"""Tools for Machine Learning training paradigms."""

import itertools
import logging
from collections.abc import Generator
from enum import Enum
from typing import Any, Self

import matplotlib.pyplot
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn
import sklearn.base
from imblearn.over_sampling import SMOTE
from numpy.typing import ArrayLike, NDArray
from pymodules.matplotlib_utils import remove_spines
from pymodules.misc_utils import invert_mapping
from pymodules.numpy_utils import find_nearest
from scipy.stats import linregress
from sklearn.metrics import (
    accuracy_score,
    auc,
    average_precision_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_recall_curve,
    r2_score,
    roc_curve,
)
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.utils.multiclass import unique_labels

logger = logging.getLogger(__name__)


def leave_one_identifier_out_cv(
    data_df: pd.DataFrame, identifier: str, identifier_isolation: bool = False
) -> Generator[tuple[NDArray[Any], NDArray[Any]], None, None]:
    """A generic cross-validation utility for generation of flexible
    leave-one-identifier-out cross-validation folds.

    Combination of identifiers can be specified by concatenating
    corresponding level names with the "+" character.

    Setting identifier_isolation=True ensures that identifier levels of the validation
    fold are not used in the training set.

    Args:
        data_df:
            A Pandas DataFrame with identifiers as index level, and columns as features.
        identifier:
            A string specifying the index level along which to perform the leave-one-out paradigm.
        identifier_isolation:
            If True, ensure that none of the identifier levels present in the validation fold are
            used in the training set.

    Yields:
        train and tests indices for each split
    """

    cv_df = data_df.copy()
    id_bits = identifier.split("+")
    array_idx = np.arange(len(cv_df))

    # Check the identifier exists
    if identifier not in cv_df.index.names:
        if len(id_bits) > 1:
            levels = [cv_df.index.get_level_values(id_bit) for id_bit in id_bits]
            cv_df[identifier] = ["+".join(id_bits) for id_bits in zip(*levels, strict=True)]
            cv_df = cv_df.set_index(identifier, append=True)
        else:
            raise ValueError(
                f"The identifier {identifier} could not be found. To specify "
                "a composite identifier, use the + character"
            )

    # Fold generation loop
    for validation_fold in cv_df.index.unique(identifier):
        validation_fold_bits = validation_fold.split("+")

        if identifier_isolation:
            training_fold = [
                [fold for fold in cv_df.index.unique(id_bit) if fold != validation_fold_bit]
                for id_bit, validation_fold_bit in zip(id_bits, validation_fold_bits, strict=True)
            ]
        else:
            training_fold = [
                [fold for fold in cv_df.index.unique(identifier) if fold != validation_fold]
            ]

        # Get the training boolean array
        training_boolean = np.array(
            [
                [label in training_fold_id for label in cv_df.index.get_level_values(id_bit)]
                for id_bit, training_fold_id in zip(id_bits, training_fold, strict=True)
            ]
        ).all(axis=0)
        # Get the testing boolean array
        testing_boolean = cv_df.index.get_level_values(identifier) == validation_fold

        # Make sure train and validation folds are mutually exclusive
        if sum(training_boolean & testing_boolean) > 0:
            raise ValueError("Training & testing folds are not exclusive, check your dataset ! ")

        # Yield the train and tests indices
        train_idx = array_idx[training_boolean]
        test_idx = array_idx[testing_boolean]

        yield train_idx, test_idx


def plot_pred_error(
    y_test: ArrayLike,
    y_pred: ArrayLike,
    best_fit: bool = True,
    title: str | None = None,
) -> matplotlib.axes.Axes:
    """Plots the actual targets from the dataset against the predicted
    values generated by a model. This plot is useful to detect noise or
    heteroscedasticity along a range of the target domain.

    See https://www.scikit-yb.org/en/latest/api/regressor/peplot.html

    Args:
        y_test:
            An array-like of the ground truth of tests data
        y_pred:
            An array-like of the prediction on the tests set
        best_fit:
            A boolean indicating if a best fit line should be computed

    Returns:
        A Matplotlib axes object
    """

    # Compute R2
    r2 = r2_score(y_test, y_pred)

    fig, ax = plt.subplots()
    # plot ground truth and prediction
    label = f"$R^2 = {r2:.3f}$"
    ax.scatter(y_test, y_pred, label=label)
    ax.set_ylabel(r"$\hat{y}$")
    ax.set_xlabel(r"$y$")

    # Get the current limits
    ylim = ax.get_ylim()
    xlim = ax.get_xlim()

    # Find the range that captures all data
    bounds = (
        min(ylim[0], xlim[0]),
        max(ylim[1], xlim[1]),
    )

    # Plot the diagonal line
    ax.plot(
        bounds,
        bounds,
        linewidth=1,
        color="black",
        linestyle="--",
        label="identity",
    )

    # Reset the limits
    ax.set_xlim(bounds)
    ax.set_ylim(bounds)
    if title:
        ax.set_title(title)

    # Ensure the aspect ratio is square
    ax.set_aspect("equal", adjustable="box")

    # Draw a linear best fit line to estimate the correlation between the
    # predicted and measured value of the target variable
    if best_fit:
        slope, intercept, r_value, p_value, std_err = linregress(y_test, y_pred)
        x = y_test
        y = y_test * slope + intercept
        ax.plot(
            x,
            y,
            alpha=0.75,
            linewidth=1,
            color="black",
            linestyle="-",
            label="best fit",
        )

    remove_spines()
    ax.legend()

    return ax


def get_binary_metrics(y_test: ArrayLike, y_pred: ArrayLike) -> dict[str, Any]:
    """Computes performance metrics for a binary classification task."""

    if (n_labels := len(np.unique(y_pred))) > 2:  # noqa: PLR2004
        raise ValueError(f"Number of labels = {n_labels} is not a binary classification task")

    # Specificity = spc = tn/n
    tn = np.sum(np.logical_and(y_test == 0, y_pred == 0))  # Sum of negative cases
    # identified as negative
    n = np.sum(y_test == 0)  # Sum of the negative cases
    spc = tn / n

    # Sensitivity = recall = tpr = tp/p
    tp = np.sum(np.logical_and(y_test == 1, y_pred == 1))
    p = np.sum(y_test == 1)  # Sum of the positive cases
    tpr = tp / p

    # Precision = ppv = tp/(tp+fp)
    ppv = tp / np.sum(y_pred == 1)

    # Fall-out = fpr = fp/n = 1 - spc
    fpr = 1 - spc

    roc_measures = {"recall": tpr, "specificity": spc, "precision": ppv, "fall_out": fpr}

    return roc_measures


def plot_feature_importances(
    ensemble_model: sklearn.ensemble.BaseEnsemble,
    features_dict: dict[str, Any] | None = None,
    n_features: int = 10,
    ax: matplotlib.axes.Axes | None = None,
) -> tuple[matplotlib.figure.Figure, NDArray[Any]]:
    """Plots feature importance from a ensemble model."""

    if ax is None:
        fig, ax = plt.subplots()
    else:
        fig = ax.get_figure()  # type: ignore
        plt.sca(ax)  # Set the axis to be the current axis

    feature_importances = ensemble_model.feature_importances_
    if len(feature_importances) < n_features:
        nfeatures = len(feature_importances)

    std = np.std(
        [tree.feature_importances_ for tree in ensemble_model.estimators_],
        axis=0,
    )
    indices = np.argsort(feature_importances)[::-1]

    # Retrieve the labels if a mapping is provided
    if features_dict:
        feature_names = np.asarray([invert_mapping(features_dict)[key] for key in indices])
    else:
        feature_names = np.arange(0, len(indices))

    # Plot the feature importances of the forest
    ax.bar(
        range(nfeatures),
        feature_importances[indices[0:nfeatures]],
        color="r",
        yerr=std[indices[0:nfeatures]],
        align="center",
    )

    ax.set_title("Feature importances")
    ax.set_xticks(range(nfeatures))
    ax.set_xticklabels(
        [feature_names[x] for x in indices[0:nfeatures]],
        rotation=45,
        ha="right",
        va="top",
        rotation_mode="anchor",
    )

    # Remove spines
    remove_spines(ax, "nobox")

    return fig, indices


def plot_confusion_matrix(
    y_true: ArrayLike,
    y_pred: ArrayLike,
    print_score: bool = True,
    title: str | None = None,
    labels: list[str] | None = None,
    normalize: bool = False,
    colorbar: bool = False,
    ax: matplotlib.axes.Axes | None = None,
    cmap: str | matplotlib.colors.Colormap = "Blues",
) -> matplotlib.axes.Axes:
    """Prints and plots a confusion matrix.

    Normalization can be applied by setting `normalize=True`.
    """

    if ax is None:
        fig, ax = plt.subplots()
    else:
        fig = ax.get_figure()  # type: ignore
        plt.sca(ax)  # Set the axis to be the current axis

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    if normalize:
        cm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]

    # Only use the labels that appear in the data
    if labels:
        classes = labels
    else:
        classes = unique_labels(y_true, y_pred)

    if title is None:
        if normalize:
            title = "Normalized confusion matrix"
        else:
            title = "Confusion matrix"

    if print_score:
        title = f"{title} (acc = {accuracy_score(y_true, y_pred)}:.2f)"

    im = ax.imshow(cm, interpolation="nearest", cmap=cmap)
    ax.set_title(title)
    if colorbar:
        fig.colorbar(im, fraction=0.046, pad=0.04)
    else:
        pass
    ax.set_xticks(np.arange(len(classes)))
    ax.set_xticklabels(classes, rotation=45, ha="right", rotation_mode="anchor")
    ax.set_yticks(np.arange(len(classes)))
    ax.set_yticklabels(classes, rotation=45, ha="right", rotation_mode="anchor")

    # We need to adjust the ylimit
    ax.set_ylim((len(classes) - 0.5, -0.5))

    # Annotation
    fmt = ".2f" if normalize else "d"
    thresh = np.ceil(cm.max() / 2)
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        ax.text(
            j,
            i,
            format(cm[i, j], fmt),
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black",
        )

    ax.set_ylabel("True label")
    ax.set_xlabel("Predicted label")

    return ax


def compute_roc(y_true: ArrayLike, y_score: ArrayLike) -> tuple[float, float, float]:
    """Computes ROC curve and area under the curve."""

    fpr, tpr, _ = roc_curve(y_true, y_score)
    score = auc(fpr, tpr)

    return fpr, tpr, score


def compute_precision_recall_metrics(
    y_true: ArrayLike, y_score: ArrayLike, metrics: str = "mAP"
) -> tuple[float, float, float]:
    """Computes recall and precision curves,
    and a score metrics (mean average precision or area under the curve)."""

    precision, recall, _ = precision_recall_curve(y_true, y_score)
    # If the calculated recall and precision scores include a pair of (
    # precision=0, recall=0). There will be two conflicting precision values
    # for recall=0 (0 and 1), which could lead to artifacts.
    precision[np.nonzero(recall == 0)[0]] = 1

    if metrics == "auc":
        score = auc(recall, precision)
    elif metrics == "mAP":
        # We can also calculate the mean precision score
        score = average_precision_score(y_true, y_score, average="macro")

    return recall, precision, score


def roc_plot(ground_truth: ArrayLike, y_probas: ArrayLike) -> matplotlib.figure.Figure:
    """Plots a simple ROC curve."""

    # Get roc statistics
    fpr, tpr, roc_auc = compute_roc(ground_truth, y_probas)

    # Plot them
    fig, ax = plt.subplots()
    # Plot the curve
    ax.plot(fpr, tpr, lw=1, alpha=0.5, label=f"ROC curve (AUC = {roc_auc:0.2f}")

    # Chance line
    ax.plot(
        [0, 1],
        [0, 1],
        linestyle="--",
        lw=1,
        color="r",
        label="Chance level",
        alpha=0.8,
    )
    ax.set_xlim(-0.05, 1.05)
    ax.set_ylim(-0.05, 1.05)
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate")
    ax.set_title("Receiver operating characteristic curve")
    ax.legend(loc="lower right")
    ax.set_aspect("equal", adjustable="box")
    remove_spines(ax, "nobox")

    return fig


def adjust_class_pred(y_scores: ArrayLike, t: float) -> list[int]:
    """Adjusts class predictions based on the prediction threshold (t)."""

    return [1 if y >= t else 0 for y in y_scores]  # type: ignore


def precision_recall_threshold(
    y_true: ArrayLike, y_score: ArrayLike, precision_threshold: float | None = None
) -> tuple[matplotlib.axes.Axes, matplotlib.figure.Figure]:
    """Returns the confusion matrix of class predictions adjusted to a given precision threshold."""

    precision, recall, thresholds = precision_recall_curve(y_true, y_score)
    # Find the best recall/precision compromise
    if precision_threshold:
        logger.info(f"Finding optimal threshold with precision= {precision_threshold:.2f}")
        precision_tr = precision[:-1]
        recall_tr = recall[:-1]
        if sum(precision_tr >= precision_threshold) == 0:
            precision_threshold = find_nearest(precision_tr, precision_threshold)
            logger.info(f"Threshold is too stringent, closest is {precision_threshold:.2f}")

        precision_mask = precision_tr >= precision_threshold
        threshold_idx = np.flatnonzero(precision_mask)
        max_recall_idx = threshold_idx[np.argmax(recall_tr[precision_mask])]

        t = thresholds[max_recall_idx]
    else:
        t = 0.5
        precision_threshold = 0.5

    # Convert continuous score to a discrete prediction
    adjusted_y_pred = adjust_class_pred(y_score, t)

    # Print the confusion matrix
    conf_mat_fig = plot_confusion_matrix(y_true, adjusted_y_pred)
    logger.info(classification_report(y_true, adjusted_y_pred))

    # plot the precision/recall curve
    curve_fig, ax = plt.subplots()
    ax.plot(
        recall,
        precision,
        color="b",
        label=f"F1-score = {f1_score(y_true, adjusted_y_pred):.2f}",
        lw=1,
        alpha=0.8,
    )

    # plot the current threshold on the line
    close_default_clf = np.argmin(np.abs(thresholds - t))
    ax.axvline(
        recall[close_default_clf],
        c="r",
        linestyle="--",
        label=f"threshold={t:.2f}",
    )
    ax.axhline(precision[close_default_clf], c="k", linestyle="--")
    # Set aspect
    ax.set_xlim(-0.05, 1.05)
    ax.set_ylim(-0.05, 1.05)
    ax.set_xlabel("Recall")
    ax.set_ylabel("Precision")
    ax.set_title(f"Precision and Recall curve threshold at precision={precision_threshold:.2f}")
    ax.set_aspect("equal", adjustable="box")
    remove_spines(ax, "nobox")
    ax.legend(loc="lower right")

    return conf_mat_fig, curve_fig


class ScalerType(str, Enum):
    MIN_MAX = "min_max"
    ZSCORE = "zscore"


class BinaryClassifierAnalyzer:
    """Class to analyse cross-validation and selectivity/sensitivity
    thresholds in a binary classification task.

    Attributes:
        classifier:
            A scikit-learn instance
        X:
            A feature matrix
        y:
            A label matrix
        cv:
            A scikit-learn cross-validator instance
        scale:
            A string to specify scaling type (minmax or zscore)
        equalize_class:
            A boolean indicating if oversampling should be performed to equalize classes
    """

    def __init__(
        self,
        classifier: sklearn.base.BaseEstimator,
        X: ArrayLike,
        y: ArrayLike,
        cv: sklearn.model_selection.BaseCrossValidator = None,
        scale: str | None = None,
        equalize_class: bool = False,
        verbose: bool = False,
    ):
        # Parse initialization arguments
        self.classifier = classifier

        # Prepare X and y data
        self.X = np.asarray(X)
        self.y = np.asarray(y)

        self.feature_names = [f"Feature n°{i}" for i in np.arange(self.X.shape[1])]
        if len(self.y.shape) > 1:
            y_dim = self.y.shape[1]
        else:
            y_dim = 1
        self.target_names = [f"Target n°{i}" for i in np.arange(y_dim)]

        # Cross validator
        if not cv:
            self.cv = StratifiedKFold()
        else:
            self.cv = cv
        # Scaler
        if ScalerType(scale) is ScalerType.MIN_MAX:
            self.scaler = MinMaxScaler()
        elif ScalerType(scale) is ScalerType.ZSCORE:
            self.scaler = StandardScaler()
        else:
            self.scaler = None

        self.equalize_class = equalize_class
        self.verbose = verbose

        # Initialize some parameters
        self.ntargets = self.y.shape[-1]

        # Cross validate at initialization
        self.cross_validate()

    def cross_validate(self) -> Self:
        """Cross-validation loop"""

        self.y_score_: dict[str, Any] = {k: [] for k in self.target_names}
        self.ground_truth_: dict[str, Any] = {k: [] for k in self.target_names}

        # We loop through CVs and target
        for cv_fold, (train, test) in enumerate(self.cv.split(self.X, self.y), start=1):
            logger.info(
                f"Round {cv_fold}, training on {len(train)} samples, testing on {len(test)} samples"
            )
            X_train, X_test, y_train, y_test = (
                self.X[train],
                self.X[test],
                self.y[train],
                self.y[test],
            )

            # Equalize training samples. This must be done AFTER cross
            # validation splitting to avoid overfitting
            if self.equalize_class:
                logger.info("Oversampling underrepresented training data with SMOTE algorithm...")
                X_train, y_train = SMOTE().fit_resample(X_train, y_train)

            # X train and X tests definition
            if self.scaler:
                X_train = self.scaler.fit_transform(X_train)
                X_test = self.scaler.transform(X_test)

            # Fit model and predict on tests set
            self.classifier.fit(X_train, y_train)
            try:
                y_score = self.classifier.predict_proba(X_test)
            except AttributeError:
                y_score = self.classifier.decision_function(X_test)

            # Loop through targets
            for i, target in enumerate(self.target_names):
                if isinstance(y_score, list):
                    target_score = y_score[i]
                    ground_truth = y_test[:, i]
                else:
                    target_score = y_score
                    ground_truth = y_test

                # Store the output of classification. Last column (positive
                # class if score is a probability, confidence score otherwise)
                try:
                    self.y_score_[target].append(target_score[:, 1])
                except IndexError:
                    self.y_score_[target].append(target_score)
                self.ground_truth_[target].append(ground_truth)

                # Also print classification report
                if self.verbose:
                    y_pred = self.classifier.predict(X_test)
                    logger.info(classification_report(y_test, y_pred))

        return self

    def plot_curve(self, curve: str = "roc") -> list[matplotlib.figure.Figure]:
        """Plot the curves over cross-validation folds"""
        fig_dict = {}
        x_grid = np.linspace(0, 1, 100)

        # Loop through targets and cross validation rounds
        for target in self.target_names:
            if curve == "roc":
                roc_output = [
                    compute_roc(y_true, y_score)
                    for y_true, y_score in zip(
                        self.ground_truth_[target], self.y_score_[target], strict=True
                    )
                ]
                # then transpose it using zip
                x_cv, y_cv, score_cv = zip(*roc_output, strict=True)
                # Label of the plot
                x_label = "False Positive Rate"
                y_label = "True Positive Rate"

            elif curve == "precision-recall":
                precision_recall_output = [
                    compute_precision_recall_metrics(y_true, y_score)
                    for y_true, y_score in zip(
                        self.ground_truth_[target], self.y_score_[target], strict=True
                    )
                ]
                # then transpose it using zip
                x_cv, y_cv, score_cv = zip(*precision_recall_output, strict=True)

                # AP expected by chance
                rand_mAP_cv = [
                    np.sum(y_true) / len(y_true) for y_true in self.ground_truth_[target]
                ]

                # Label of the plot
                x_label = "Recall"
                y_label = "Precision"

            fig, ax = plt.subplots()
            # Plot loop
            for i, (x, y, score) in enumerate(zip(x_cv, y_cv, score_cv, strict=True)):
                if curve == "precision-recall":
                    label = f"{curve.title()} fold {i} (mAP = {score:.2f} / {rand_mAP_cv[i]:.2f})"
                elif curve == "roc":
                    label = f"{curve.title()} fold {i} (AUC = {score:.2f})"

                ax.plot(x, y, lw=1, alpha=0.5, label=label)

            # Finalize the plots with average if it is a roc plot
            if curve == "roc":
                # Interpolate each fold to be able to average them, set the
                # boundary
                interp_y_cv = [np.interp(x_grid, x, y) for x, y in zip(x_cv, y_cv, strict=True)]
                mean_x = x_grid
                mean_y = np.mean(interp_y_cv, axis=0)
                # Constraint the interpolation to 0,1 range
                mean_y[0] = 0.0
                mean_y[-1] = 1.0
                ax.plot(
                    [0, 1],
                    [0, 1],
                    linestyle="--",
                    lw=1,
                    color="r",
                    label="Chance level",
                    alpha=0.8,
                )

                mean_auc = auc(mean_x, mean_y)
                std_auc = np.std(score_cv)
                ax.plot(
                    mean_x,
                    mean_y,
                    color="b",
                    label=rf"Mean {curve.title()} (AUC = {mean_auc:0.2f} $\pm$ {std_auc:0.2f})",
                    lw=2,
                    alpha=0.8,
                )

                std_y = np.std(interp_y_cv, axis=0)
                y_upper = np.minimum(mean_y + std_y, 1)
                y_lower = np.maximum(mean_y - std_y, 0)
                ax.fill_between(
                    mean_x,
                    y_lower,
                    y_upper,
                    color="grey",
                    alpha=0.2,
                    label=r"$\pm$ 1 std. dev.",
                )

            # Finally set the axis
            ax.set_xlim(-0.05, 1.05)
            ax.set_ylim(-0.05, 1.05)
            ax.set_xlabel(x_label)
            ax.set_ylabel(y_label)
            ax.set_title(f"{curve.capitalize()} for {target}")
            ax.legend(loc="lower right")
            ax.set_aspect("equal", adjustable="box")
            remove_spines(ax, "nobox")

            fig_dict[target] = fig

        return list(fig_dict.values())
